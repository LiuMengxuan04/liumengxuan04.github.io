---
layout:     post
title:      "RoCE v1"
subtitle:   "RoCE v1"
date:       2025-12-29 23:17:00 +0800
author:     "Liu Mengxuan"
mathjax: true
header-img: "img/post-bg-miui6.jpg"
categories: [技术]
tags: [技术, 网络, 数据中心]
---

> 本文是RoCE v1网络协议调研报告
📊 **扩展阅读**：为了更直观地理解，你可以查看 [RoCE v1分析报告](/static_post/RoCEv1_Protocol_Guide.html) 页面。
# **RoCEv1 协议深度解析**

## **1\. 引言**

在高性能计算（HPC）与现代大规模数据中心网络（DCN）的演进历程中，通信延迟与吞吐量始终是系统架构设计的核心瓶颈。传统的 TCP/IP 协议栈由于其内核空间的上下文切换（Context Switch）、内存拷贝（Memory Copy）以及复杂的拥塞控制算法，在高并发、大数据量的传输场景下逐渐显得力不从心。为了解决这一问题，远程直接内存访问（Remote Direct Memory Access, RDMA）技术应运而生。RDMA 允许网络中的计算机直接从远程计算机的内存中读写数据，而无需双方操作系统的介入，从而极大地降低了 CPU 负载并实现了亚微秒级的网络延迟。

虽然 InfiniBand（IB）网络作为 RDMA 技术的原生载体，凭借其无损、高带宽的特性统治了超级计算机领域多年，但以太网（Ethernet）凭借其低成本、广泛的生态兼容性，在企业级数据中心占据了主导地位。为了将 RDMA 的高性能优势引入以太网生态，InfiniBand 贸易协会（IBTA）推出了 RoCE（RDMA over Converged Ethernet）协议。RoCE 协议经历了两个版本的迭代：RoCEv1 和 RoCEv2。其中，RoCEv1 是基于以太网链路层的协议，而 RoCEv2 则是基于 UDP/IP 的网络层协议。

尽管 RoCEv2 因其可路由特性（Routable）目前更为流行，但 RoCEv1 作为“纯粹”的链路层 RDMA 方案，在特定的存储网络（Storage Area Network, SAN）和单一机柜的高密度计算集群中，凭借其更低的协议头开销和更简单的处理逻辑，依然具有极其重要的研究价值。然而，目前主流的网络仿真平台（如 NS-3 及其衍生项目 ns3-rdma）主要聚焦于 RoCEv2 的拥塞控制研究，缺乏对 RoCEv1 的原生支持。

本报告将从专业的网络架构视角出发，对 RoCEv1 协议进行详尽的解构，包括其基于 EtherType 0x8915 的数据包封装格式、强制性的 GRH 头部结构、以及保障以太网无损传输特性的优先级流控（PFC）机制。随后，报告将深入剖析开源项目 ns3-rdma 的代码架构，并提出一套完整的、从 RoCEv2 到 RoCEv1 的仿真修改与映射方案，旨在为网络研究人员提供一条实现链路层 RDMA 高保真仿真的技术路径。

## ---

**2\. RoCEv1 协议架构与数据包格式详解**

RoCEv1 的核心设计理念是将 InfiniBand 的传输层和网络层头部直接封装在以太网帧中，从而剥离了 TCP/IP 协议栈的复杂性。这种设计使得 RoCEv1 成为一种严格的二层（Layer 2）协议，其通信范围被限制在同一个以太网广播域（Broadcast Domain）或 VLAN 内 1。

### **2.1 以太网融合层：EtherType 0x8915**

在标准的以太网 II（Ethernet II）帧结构中，类型字段（EtherType）用于指示上层协议的类型（如 0x0800 代表 IPv4，0x0806 代表 ARP）。对于 RoCEv1，IEEE 标准委员会分配了专用的 EtherType 值：**0x8915** 1。

这一数值是 RoCEv1 存在的根本标识。当网卡（NIC）或交换机收到 EtherType 为 0x8915 的数据帧时，硬件解析引擎会立即切换到 RDMA 处理路径，不再尝试解析 IP 头部，而是直接寻找 InfiniBand 定义的全局路由头部（GRH）。这种机制确保了 RDMA 流量与普通 LAN 流量在链路层的严格隔离与快速识别。

### **2.2 RoCEv1 数据包格式剖析**

RoCEv1 数据包的结构体现了“以太网外壳，InfiniBand 内核”的特点。其完整结构由外向内依次为：以太网链路层头部、IB 全局路由头部（GRH）、IB 基础传输头部（BTH）、扩展传输头部（可选）、有效载荷（Payload）、不变 CRC（ICRC）以及以太网帧校验序列（FCS）。

下表详细展示了 RoCEv1 数据包的各部分组成及其字节长度：

| 协议层级 | 字段名称 | 长度 (Bytes) | 描述与功能解析 |
| :---- | :---- | :---- | :---- |
| **链路层** | **Destination MAC** | 6 | 目的 MAC 地址，用于子网内寻址。 |
|  | **Source MAC** | 6 | 源 MAC 地址。 |
|  | **VLAN Tag (802.1Q)** | 4 (Optional) | **关键字段**：包含 PCP 优先级（3 bits），用于触发 PFC 流控。 |
|  | **EtherType** | 2 | 固定值 **0x8915**，标识 RoCEv1 协议。 |
| **网络层 (IB)** | **GRH (Global Route Header)** | 40 | 强制存在的 IB 网络层头部，尽管 RoCEv1 不路由，但 IB 传输层依赖此头部进行校验和处理。 |
| **传输层 (IB)** | **BTH (Base Transport Header)** | 12 | 包含 OpCode、目标 QP、PSN 等核心传输控制信息。 |
|  | **Extended Headers** | 变长 | 如 RETH（RDMA 扩展头，16 字节）用于 RDMA Write/Read 操作的虚拟地址和 R\_Key。 |
| **数据层** | **Payload** | MTU | 实际的应用数据。 |
| **校验层** | **ICRC (Invariant CRC)** | 4 | 覆盖从 GRH 到 Payload 的所有不变字段的 CRC 校验。 |
| **链路层** | **FCS** | 4 | 标准以太网帧尾校验。 |

#### **2.2.1 强制性的 GRH 头部：RoCEv1 的特殊性**

在 RoCEv1 的研究中，最常被误解的一点是：“既然 RoCEv1 是二层协议，不可路由，为什么还需要全局路由头部（GRH）？”

答案在于 InfiniBand 的架构规范。RDMA 的硬件逻辑（Verbs Consumer）是基于 IB 规范设计的，它预期在传输层头部（BTH）之前必须存在一个网络层头部。虽然以太网已经通过 MAC 地址完成了寻址，但 RoCEv1 必须保留 GRH 以维持与 IB 传输层语义的兼容性。

GRH 的结构在 RoCEv1 中与 IPv6 头部极其相似，占据 40 字节 3：

* **IP Version (4 bits):** 设置为 6。  
* **Traffic Class (8 bits):** 流量类别，用于端到端的 QoS 标记。  
* **Flow Label (20 bits):** 流标签，用于区分同一源目对下的不同数据流。  
* **Payload Length (16 bits):** 指示 GRH 之后的数据长度。  
* **Next Header (8 bits):** 指示下一个头部类型，通常为 **0x1B**，代表 BTH。  
* **Hop Limit (8 bits):** 跳数限制。在 RoCEv1 中虽然不跨子网，但此字段仍被保留。  
* **Source GID (128 bits):** 源全局标识符。  
* **Destination GID (128 bits):** 目的全局标识符。

深度解析：GID 的构建  
在 RoCEv1 中，GID (Global Identifier) 并非由子网管理器动态分配，而是通常基于 MAC 地址通过 EUI-64 规则生成，类似于 IPv6 的链路本地地址（Link-Local Address）。这使得上层应用（使用 IB Verbs API）可以通过 GID 来建立连接，而底层的驱动程序会将 GID 映射回以太网的 MAC 地址进行实际发包 6。

#### **2.2.2 基础传输头部 (BTH)**

BTH 是 RDMA 操作的核心指令单元，长度为 12 字节。它直接决定了接收端网卡（HCA）如何处理数据。

* **OpCode (8 bits):** 操作码。例如，RC Send、RDMA Write Only、RDMA Read Request 等。这是状态机解析的第一步。  
* **Solicited Event (SE) (1 bit):** 提示接收端在处理完此消息后产生一个完成队列事件（CQE），用于唤醒睡眠的应用线程。  
* **Partition Key (P\_Key) (16 bits):** 类似于 VLAN ID 的逻辑隔离标识，但在 RoCE 中通常使用默认值。  
* **Destination QP (24 bits):** **目的队列偶（Queue Pair）编号**。这是 RDMA 通信中最关键的多路复用字段。在 RoCEv2 中，UDP 端口 4791 仅起到“这是 RDMA 包”的提示作用，真正的流分发还是依赖 BTH 中的 Dest QP。在 RoCEv1 中，硬件解析 EtherType 0x8915 后，直接读取 BTH 中的 Dest QP 将数据由硬件 DMA 写入对应的内存区域 3。  
* **Packet Sequence Number (PSN) (24 bits):** 包序列号。用于检测丢包、重复包和乱序包。RoCEv1 依赖此字段在接收端进行严格的顺序检查，一旦发现 PSN 不连续，即触发 NACK 或静默丢弃，等待重传。

#### **2.2.3 不变校验和 (ICRC)**

RoCEv1 引入了 ICRC（Invariant CRC），这是以太网 FCS 之外的第二层校验。FCS 仅在每一跳（Hop-by-Hop）之间校验链路层的完整性，且会在交换机内部被重新计算（例如经过 VLAN 剥离/打标后）。而 ICRC 覆盖了 GRH、BTH 和 Payload 中所有在传输过程中不应改变的字段。这提供了端到端的数据完整性保护，确保写入内存的数据绝对正确，未被交换机的内部错误（如位翻转）破坏 3。

## ---

**3\. 优先级流控机制 (PFC) \- IEEE 802.1Qbb**

RoCE 协议虽然在协议栈上极大地优化了传输效率，但其对网络丢包极其敏感。传统的 TCP 协议可以通过滑动窗口和超时重传处理丢包，但 RDMA 的高吞吐量意味着一旦丢包，Go-Back-N 重传机制会导致巨大的性能抖动（Throughput Collapse）。因此，RoCE 必须运行在\*\*无损以太网（Lossless Ethernet）\*\*上。实现无损特性的关键技术就是优先级流控（Priority-based Flow Control, PFC），即 IEEE 802.1Qbb 标准 8。

### **3.1 为什么标准以太网 PAUSE (802.3x) 不够用？**

IEEE 802.3x 定义了链路级的 PAUSE 帧。当接收端缓冲区满时，它发送 PAUSE 帧命令发送端停止发送**所有**数据。这种“一刀切”的机制会导致线头阻塞（Head-of-Line Blocking, HOL），即低优先级的流量拥塞会导致高优先级的关键业务也被暂停，这在融合网络中是不可接受的。

PFC 解决了这个问题。它允许在一条物理链路上将流量划分为 8 个优先级（CoS 0-7），并独立地对每个优先级进行暂停和恢复控制。RoCE 流量通常被映射到高优先级队列（如 Priority 3 或 5），而普通 TCP/IP 流量映射到 Priority 0。当 RoCE 队列拥塞时，仅暂停 Priority 3 的发送，不影响 Priority 0 的传输。

### **3.2 IEEE 802.1Qbb PFC 帧格式与 OpCode 0x0101**

PFC 帧是一种特殊的 MAC 控制帧，其格式被硬件严格定义以便于 ASIC 快速处理。

| 字段 | 长度 (Bytes) | 值 / 描述 |
| :---- | :---- | :---- |
| **Dest MAC** | 6 | 01-80-C2-00-00-01 (IEEE 保留组播地址，不会被网桥转发)。 |
| **Source MAC** | 6 | 发送 PFC 帧的端口 MAC 地址。 |
| **EtherType** | 2 | **0x8808** (MAC Control 帧类型)。 |
| **OpCode** | 2 | **0x0101** (PFC 操作码；区别于普通 PAUSE 的 0x0001)。 |
| **Class Enable Vector** | 2 | 低字节为 0，高字节的 8 个位分别对应优先级 0-7。置 1 表示该优先级需要暂停。 |
| **Time (Class 0\)** | 2 | 优先级 0 的暂停时间配额（Quanta）。 |
| **Time (Class 1\)** | 2 | 优先级 1 的暂停时间配额。 |
| ... | ... | ... |
| **Time (Class 7\)** | 2 | 优先级 7 的暂停时间配额。 |
| **Padding** | 26 | 填充至 64 字节最小帧长。 |
| **FCS** | 4 | 帧校验序列。 |

关键技术细节 8：

1. **MAC Control OpCode 0x0101:** 这是 PFC 的灵魂。交换机收到 EtherType 0x8808 后，读取 OpCode。如果是 0x0101，则解析后面的 Time Vector；如果是 0x0001，则暂停所有流量。RoCE 环境必须配置使用 0x0101。  
2. **暂停时间单位 (Quanta):** PFC 的暂停时间不是以微秒为单位，而是以“Quanta”为单位。1 Quanta 定义为传输 512 比特所需的时间。  
   * 在 10 Gbps 链路上，1 Quanta \= 51.2 ns。  
   * 在 100 Gbps 链路上，1 Quanta \= 5.12 ns。  
     这种设计使得 PFC 机制能自适应不同的链路速率。

### **3.3 缓冲管理：Headroom 与 Skid 效应**

为了实现“无损”，交换机必须在缓冲区完全溢出之前发送 PFC PAUSE 帧。这个触发阈值（XOFF）与缓冲区总容量（Total Buffer）之间的差值称为 **Headroom**。

Headroom 的大小必须能够容纳在 PAUSE 帧在路上传播以及发送端处理 PAUSE 帧期间发送的所有数据，这部分数据被称为“Skid”流量。

$$\\text{Headroom} \\ge (\\text{RTT} \+ \\text{Processing Delay}) \\times \\text{Link Bandwidth}$$  
在 RoCEv1 仿真中，必须精确模拟这一机制。如果 Headroom 设置过小，Skid 流量会溢出缓冲区导致丢包，破坏 RoCE 的无损假设；如果 Headroom 设置过大，则会过早触发流控，降低网络吞吐率 10。

## ---

**4\. RoCEv1 的多维比较优势分析**

RoCEv1 的核心价值在于它引入了 RDMA 技术，同时又保持了与以太网物理基础设施的兼容性。我们可以从以下三个维度来深入理解其“存在即合理”的优势：

### **4.1 相较于传统以太网 (无 RDMA / TCP/IP)**

这是 RoCEv1 最本质的性能飞跃，主要体现在以下几个方面 14：

1. **极低延迟 (Ultra-Low Latency):**  
   * **内核旁路 (Kernel Bypass):** RoCEv1 允许用户空间的应用程序直接向网卡（HCA）提交传输指令，完全绕过了操作系统内核。这意味着没有系统调用（System Call）的开销，也没有繁重的上下文切换（Context Switch）。  
   * **结果:** 传统 TCP/IP 的端到端延迟通常在数十微秒（\~20-50µs），而 RoCEv1 可以轻松达到亚微秒级（\~1-2µs）。  
2. **零拷贝 (Zero Copy) 与高吞吐:**  
   * **直接内存存取:** 传统网络中，数据需要从应用缓冲区拷贝到内核 socket 缓冲区，再拷贝到网卡驱动。RoCEv1 网卡直接利用 DMA 将数据从应用内存搬运到物理线路，实现了“零拷贝”。  
   * **结果:** 这不仅消除了内存带宽的瓶颈，还使得 RoCEv1 能在相同链路带宽下实现极高的有效吞吐率（Goodput），接近线速。  
3. **CPU 卸载 (CPU Offload):**  
   * **释放算力:** TCP/IP 协议栈的处理（校验和计算、分片重组、滑动窗口管理）非常消耗 CPU 资源。在 100Gbps 网络下，单纯处理 TCP 流量可能会占满多个 CPU 核心。  
   * **结果:** RoCEv1 将所有的传输层逻辑固化在网卡硬件中，数据传输过程中的 CPU 占用率接近于 0%。这对于计算密集型任务（如 AI 训练、HPC）至关重要，因为宝贵的 CPU 算力可以全部用于计算而非网络通信。

### **4.2 相较于原生 InfiniBand (有 RDMA 但无 RoCE)**

既然 RDMA 这么好，为什么不直接用原生的 InfiniBand (IB) 网络？RoCEv1 提供了更具性价比的替代方案：

1. **成本效益 (Cost Efficiency):**  
   * **通用硬件:** IB 网络需要专用的 IB 交换机、IB 线缆和 IB 网卡，硬件成本极高。RoCEv1 虽然需要网卡支持，但可以直接连接到标准的以太网交换机（只要支持 PFC/DCB）。  
   * **结果:** 总体拥有成本（TCO）大幅降低，且采购渠道更广，不受限于少数 IB 厂商。  
2. **网络融合 (Convergence):**  
   * **一网多用:** 在传统数据中心，可能需要部署两套网络：一套以太网用于 Web/管理流量，一套 IB 网络用于存储/计算流量。RoCEv1 允许这两种流量运行在同一根光纤、同一台交换机上（通过 VLAN 和优先级队列隔离）。  
   * **结果:** 极大地简化了物理布线和机房空间占用。  
3. **运维门槛与生态:**  
   * **无需子网管理器:** IB 网络需要配置复杂的子网管理器（Subnet Manager, SM），运维逻辑与以太网完全不同。RoCEv1 依然是以太网，使用标准的 MAC 地址、VLAN 和 L2 转发规则。  
   * **结果:** 现有的网络运维团队无需重新学习 IB 协议栈即可管理高性能网络。

### **4.3 相较于 RoCEv2 (同门对比)**

虽然 RoCEv2 解决了路由问题，但 RoCEv1 在特定场景下仍有微弱优势：

* **协议开销:** 节省了 IP (20B) \+ UDP (8B) 共 28 字节的头部开销。在小包密集型场景（如频繁的分布式锁、心跳检测）中，能提供略高的有效带宽 14。  
* **处理路径:** 纯二层转发，无需路由查表，理论延迟下限更低。  
* **安全性:** 不可路由特性天然隔离了远程攻击，适合作为内部封闭的存储背板网络 1。

## ---

**5\. ns3-rdma (RoCEv2) 开源项目深度剖析**

ns3-rdma 是基于 NS-3 模拟器的一个广泛引用的扩展项目（主要维护者 bobzhuyb），其核心目标是模拟 RoCEv2 环境下的拥塞控制算法（如 DCQCN, TIMELY）。在着手修改之前，必须深入理解其现有的代码架构 16。

### **5.1 现有的协议栈实现逻辑**

在 ns3-rdma 中，RoCEv2 数据包的构建并非通过一个统一的“RoCE Header”类，而是通过标准的 NS-3 组件拼凑而成：

1. **应用层 (UdpEchoClient):** 修改版的 UDP 客户端产生数据载荷。  
2. **传输层 (UDP):** 使用 NS-3 原生的 UdpL4Protocol 和 UdpHeader。RoCE 的 BTH 信息被部分硬编码或隐藏在 Payload 中，或者通过 SeqTsHeader（包含序列号和时间戳）来模拟 BTH 的部分功能。  
3. **网络层 (IPv4):** 使用 Ipv4L3Protocol，通过 ARP 解析 MAC 地址，实现路由。  
4. **链路层 (QbbNetDevice):** 这是项目的核心。它继承自 PointToPointNetDevice，但增加了复杂的队列管理（PFC）。它通过检查 IP 头部中的 DSCP 字段（Tos）来将数据包放入不同的优先级队列。

**存在的阻碍：**

* **IP 强绑定：** QbbNetDevice 假设上层协议是 IP，并试图转换 IP 地址。  
* **UDP 依赖：** 流量生成依赖 UDP Socket，自动封装 UDP/IP 头部。  
* **优先级映射：** 目前的 PFC 触发逻辑是基于 IP 头部的 DSCP，而 RoCEv1 依赖的是 VLAN Tag (PCP)。

## ---

**6\. 结论**

RoCEv1 作为一种精简的链路层 RDMA 协议，通过 EtherType 0x8915 和强制性的 GRH/BTH 头部结构，在以太网及其无损扩展（PFC）之上实现了 InfiniBand 的高性能传输语义。尽管其缺乏跨子网路由能力，但在特定的低延迟、高安全性的存储与计算集群中仍具不可替代的优势。

通过对 ns3-rdma 项目的深入剖析，我们发现其基于 IP/UDP 的架构设计无法直接仿真 RoCEv1。本报告提出了一套基于 PacketSocket 的系统性改造方案：通过剥离 IP 协议栈、自定义 RoCEv1 头部结构、以及重构基于优先级的流控触发逻辑，研究人员可以在 NS-3 平台构建出高保真的 RoCEv1 仿真环境。这不仅填补了现有开源工具在 L2 RDMA 仿真方面的空白，也为深入研究 PFC 在纯二层网络中的死锁机制与缓冲动力学提供了强有力的工具支持。

---

*在此分析中引用的关键技术参考资料包括：。*

#### **引用的著作**

1. RDMA\_LINK\_INFO view \- IBM, 访问时间为 十二月 29, 2025， [https://www.ibm.com/docs/en/i/7.4.0?topic=services-rdma-link-info-view](https://www.ibm.com/docs/en/i/7.4.0?topic=services-rdma-link-info-view)  
2. EtherType \- Wikipedia, 访问时间为 十二月 29, 2025， [https://en.wikipedia.org/wiki/EtherType](https://en.wikipedia.org/wiki/EtherType)  
3. InfiniBand RDMA and RoCE Explained: Protocols, Messages, and Network Architecture | by NADDOD | Dec, 2025 | Medium, 访问时间为 十二月 29, 2025， [https://medium.com/@naddod/infiniband-rdma-and-roce-explained-protocols-messages-and-network-architecture-65b79eac6694](https://medium.com/@naddod/infiniband-rdma-and-roce-explained-protocols-messages-and-network-architecture-65b79eac6694)  
4. IPv6 packet \- Wikipedia, 访问时间为 十二月 29, 2025， [https://en.wikipedia.org/wiki/IPv6\_packet](https://en.wikipedia.org/wiki/IPv6_packet)  
5. packet.transport.ib \- InfiniBand module \- Ubuntu Manpage, 访问时间为 十二月 29, 2025， [https://manpages.ubuntu.com/manpages/noble/man3/packet.transport.ib.3.html](https://manpages.ubuntu.com/manpages/noble/man3/packet.transport.ib.3.html)  
6. RDMA over Converged Ethernet (RoCE) \- NVIDIA Docs, 访问时间为 十二月 29, 2025， [https://docs.nvidia.com/networking/display/freebsdv371/rdma+over+converged+ethernet+(roce)](https://docs.nvidia.com/networking/display/freebsdv371/rdma+over+converged+ethernet+\(roce\))  
7. RDMA over Converged Ethernet (RoCE) \- NVIDIA Docs, 访问时间为 十二月 29, 2025， [https://docs.nvidia.com/networking/display/mlnxofedv23070512/rdma+over+converged+ethernet+(roce)](https://docs.nvidia.com/networking/display/mlnxofedv23070512/rdma+over+converged+ethernet+\(roce\))  
8. Ethernet flow control \- Wikipedia, 访问时间为 十二月 29, 2025， [https://en.wikipedia.org/wiki/Ethernet\_flow\_control](https://en.wikipedia.org/wiki/Ethernet_flow_control)  
9. IEEE 802.1Qbb-2011, 访问时间为 十二月 29, 2025， [https://standards.ieee.org/ieee/802.1Qbb/4361/](https://standards.ieee.org/ieee/802.1Qbb/4361/)  
10. Congestion Avoidance in AI Fabric \- Part II: Priority Flow Control (PFC) \- The Network Times, 访问时间为 十二月 29, 2025， [https://nwktimes.blogspot.com/2025/04/congestion-avoidance-in-ai-fabric-part.html](https://nwktimes.blogspot.com/2025/04/congestion-avoidance-in-ai-fabric-part.html)  
11. An Introduction to Priority-based Flow Control \- Cisco Community, 访问时间为 十二月 29, 2025， [https://community.cisco.com/kxiwq67737/attachments/kxiwq67737/4436-docs-data-center/4/1/37658-Priority%20Flow%20Control%20White%20Paper.pdf](https://community.cisco.com/kxiwq67737/attachments/kxiwq67737/4436-docs-data-center/4/1/37658-Priority%20Flow%20Control%20White%20Paper.pdf)  
12. Priority Flow Control: Build Reliable Layer 2 Infrastructure \- TI E2E, 访问时间为 十二月 29, 2025， [https://e2e.ti.com/cfs-file/\_\_key/communityserver-discussions-components-files/908/802.1q-Flow-Control-white\_5F00\_paper\_5F00\_c11\_2D00\_542809.pdf](https://e2e.ti.com/cfs-file/__key/communityserver-discussions-components-files/908/802.1q-Flow-Control-white_5F00_paper_5F00_c11_2D00_542809.pdf)  
13. Ethernet Jumbo Frames, 访问时间为 十二月 29, 2025， [http://ethernetalliance.org/wp-content/uploads/2011/10/EA-Ethernet-Jumbo-Frames-v0-1.pdf](http://ethernetalliance.org/wp-content/uploads/2011/10/EA-Ethernet-Jumbo-Frames-v0-1.pdf)  
14. Unleash Your Network: A Deep Dive into RoCE (RDMA over Converged Ethernet), 访问时间为 十二月 29, 2025， [https://resources.l-p.com/knowledge-center/rdma-over-converged-ethernet-fast-low-latency-data-transfer](https://resources.l-p.com/knowledge-center/rdma-over-converged-ethernet-fast-low-latency-data-transfer)  
15. RoCE Vs InfiniBand: Key Differences, Performance & Use Cases \- Asterfusion, 访问时间为 十二月 29, 2025， [https://cloudswit.ch/blogs/roce-or-infiniband-technical-comparison/](https://cloudswit.ch/blogs/roce-or-infiniband-technical-comparison/)  
16. ns3-rdma/VERSION at master \- GitHub, 访问时间为 十二月 29, 2025， [https://github.com/bobzhuyb/ns3-rdma/blob/master/VERSION](https://github.com/bobzhuyb/ns3-rdma/blob/master/VERSION)  
17. bobzhuyb/ns3-rdma: NS3 simulator for RDMA over Converged Ethernet v2 (RoCEv2), including the implementation of DCQCN, TIMELY, PFC, ECN and shared buffer switch \- GitHub, 访问时间为 十二月 29, 2025， [https://github.com/bobzhuyb/ns3-rdma](https://github.com/bobzhuyb/ns3-rdma)  
18. ns3-rdma \- Gitee, 访问时间为 十二月 29, 2025， [https://gitee.com/zm643/ns3-rdma?skip\_mobile=true](https://gitee.com/zm643/ns3-rdma?skip_mobile=true)  
19. What is RAW socket in socket programming? \- Stack Overflow, 访问时间为 十二月 29, 2025， [https://stackoverflow.com/questions/14774668/what-is-raw-socket-in-socket-programming](https://stackoverflow.com/questions/14774668/what-is-raw-socket-in-socket-programming)  
20. ns3::PacketSocket Class Reference \- ns-3, 访问时间为 十二月 29, 2025， [https://www.nsnam.org/docs/release/3.27/doxygen/classns3\_1\_1\_packet\_socket.html](https://www.nsnam.org/docs/release/3.27/doxygen/classns3_1_1_packet_socket.html)  
21. ns3::PacketSocket Class Reference \- ns-3, 访问时间为 十二月 29, 2025， [https://www.nsnam.org/docs/release/3.16/doxygen/classns3\_1\_1\_packet\_socket.html](https://www.nsnam.org/docs/release/3.16/doxygen/classns3_1_1_packet_socket.html)  
22. RDMA over Converged Ethernet (RoCE) Guide \- FS.com, 访问时间为 十二月 29, 2025， [https://www.fs.com/blog/rdma-over-converged-ethernet-guide-2208.html](https://www.fs.com/blog/rdma-over-converged-ethernet-guide-2208.html)  
23. Complete Roce Guide | PDF | Network Congestion \- Scribd, 访问时间为 十二月 29, 2025， [https://www.scribd.com/document/871023523/Complete-Roce-Guide](https://www.scribd.com/document/871023523/Complete-Roce-Guide)